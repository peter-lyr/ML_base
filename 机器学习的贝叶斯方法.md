在前面关于线性建模的学习中，我们显式地在模型中增加噪声允许我们不只是完成<font color=red>点预测</font>，特别是能够量化参数估计及后续预测过程中的不确定性。
一旦在参数中考虑了不确定性，将参数当作随机变量就变得很自然了。
这就是贝叶斯思想的基础。

贝叶斯方法的硬币游戏
==============================================
贝叶斯规则：
$$p(r|y_N)=\frac{P(y_N|r)p(r)}{P(y_N)}\tag{1-1}$$
> - $r$表示正面朝上的概率（是我们要学习的参数）。
> - $N$表示我们获得到的观测数据的总数。
> - $y_N$表示$N$次投币中正面朝上的次数。

> - $p(r)$先验分布。
> - $P(y_N|r)$似然分布。
> - $P(y_N)$边缘分布。
> - $p(r|y_N)$后验分布。
$$P(y_N)=\int_Rp(y_N,r)=\int_{r=0}^{r=1} P(y_N|r)p(r)dr\tag{1-2}$$
其中后验分布是我们感兴趣的。它是根据新的证据$y_N$更新先验信念$p(r)$的结果。
为求得后验分布，我们一般根据似然分布选择先验分布。比如这里，似然分布为二项分布，则我们选择$\beta$分布。
因为$\beta$分布是当总体分布（即似然分布）为二项分布，且带估计参数为成功概率时的先验共轭分布。
对于其他的总体分布以及参数，可以另行参考。

贝叶斯方法允许我们可以用最少的样本观测值来对参数进行预测。
当然，一开始我们甚至不需要任何观测值就可以预测了，因为有先验分布嘛。
从先验分布中我们可以得到有关参数的一些信息。
有了观测数据时，我们可以依据贝叶斯规则求出后验分布。
根据后眼分布，我们可以更新对参数的认识。

每投币一次，我们做一次记录，多次以后，我们发现，参数的期望变化不大，有涨有跌；
而参数的方差总体趋势是变小的，这说明我们学到了东西，因为从后验分布图中我们可以看到，参数的可能取值范围缩小了。

不管先验信念是均匀的硬币还是有偏的硬币，当我们增加更多的数据时，先验信念会变得越来越不重要。

<font color=red>边缘似然估计</font>

对于先验的最佳方案的选择并不是全凭主观臆断：
为进一步确定$p(r)$的值，需要进一步约束条件，即
式$(1-2)$可以写成
$$P(y_N|\alpha,\beta)=\int_{r=0}^{r=1} P(y_N|r)p(r|\alpha,\beta)dr\tag{1-2}$$
这说明了通过预先给定参数$\alpha$和$\beta$，可以确定出现数据$y_N$的概率。
所以边缘似然值越高，越符合先验分布。
当然，以这种方式选择的先验知识本质上是选择最切合数据的先验。
所以，某种意义上，边缘似然值表示数据对于先验信念的支持程度，越大则越支持。

根据观测到的数据，我们可以最大化边缘似然估计，以得到在一定范围内最合适的$\alpha$和$\beta$的值。
