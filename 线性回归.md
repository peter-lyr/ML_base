前言
========================================================
机器学习入门的最好选择是线性回归。

对于不同的机器学习问题，需要不同的模型。
而模型的选择是面对机器学习问题遇到的第一个问题。
这也是最难的，很多时候需要靠前人的经验。
因此学习各种各样的常用的模型，及其常用来解决的问题是机器学习的重难点。

接下来从最简单的线性回归模型开始，学习最基础的概念，为更复杂的模型的学习打下基础。

线性回归模型——入手最基本概念
========================================================
一个可能不是特别好的线性回归模型假设：
$$\boldsymbol{t}=f(\boldsymbol{x}|\boldsymbol{\theta})=f(x|\omega_0,\omega_1)=\omega_0+\omega_1x\tag{1.1}$$
$\boldsymbol{t}=f(\boldsymbol{x}|\boldsymbol{\theta})$就是模型假设，它是抽象的表达。
$f$通常是函数的符号，这里被称为模型，
因为建立模型，就是要建立从数据集到结果集之间的映射，而函数是最常用的一种映射。

在进一步讲解这个式子的含义之前，需要明确几个其他概念：
- 找到合适的模型之后，有了数据（数据集、结果集），我们就需要从中拿出一部分，对模型进行<font color=red>训练</font>。
训练好之后，需要对模型进行<font color=red>测试</font>。
如果有问题，那就是模型还不够好，需要再进行训练。
如果没问题了，那就可以对新的数据集进行结果<font color=red>预测</font>了。
- <font color=red>训练</font>时，我们已经选择好了模型的类型，但是我们不知道其中的参数。
这时，我们需要利用已知数据，根据统计学的方法，求出参数的值。
- <font color=red>测试</font>时，我们已经知道模型的参数，
但是我们需要根据剩下的数据集，算得结果，与实际结果相比较，以此来评估这些求得的参数到底好不好。
- 而<font color=red>预测</font>时，表明参数已经被我们认为是最合适的了，这是就已经在应用了。
- 现在只需要了解即可，以后会慢慢深入的。

$\boldsymbol{x}$是用来预测结果的、新的数据。
$\boldsymbol{t}$是预测得到的结果。
两者可以是任何数据类型，可以是一个点，也可以是n维向量。
而$\boldsymbol{\theta}$是机器学习的主要任务。
有了它，预测才能够进行。
为什么$\boldsymbol{t}=f(\boldsymbol{x}|\boldsymbol{\theta})$是抽象的表达？
因为它没有给出自变量、因变量以及参数的具体形式，甚至模型的类型都可能不知道。
而$f(x|\omega_0,\omega_1)$就比较具体了，至少我们知道：
- 该模型只有两个参数；
- 自变量是一维的。

而$\omega_0+\omega_1x$就更加具体了，我们知道：
- 参数跟子变量之间是怎么运算的；
- 更具体地，我们知道了因变量和自变量之间是呈线性相关的；
- 因变量是一个一维的数值。
- 这就是我们建立的具体的模型。

所谓具体的模型，是指我们知道其对应的函数的类型，以及需要的参数。
具体的模型有很多，可能它们对应的函数的类型完全不一样（这对模型的影响是最大的），
也可能它们之间只是参数个数不同（往往参数越多，训练出来的模型越精确，但是更加地难以训练）。

为了能够统一不同模型的讲解，我们使用抽象形式的表达：
$$\boldsymbol{t}=f(\boldsymbol{x}|\boldsymbol{\theta})\tag{1.2}$$

接下来要讲解的是如何训练数据，得到最好的模型。

评价模型好坏的标准
========================================================
既然模型$\boldsymbol{t}=f(\boldsymbol{x}|\boldsymbol{\theta})$是$\boldsymbol{x}$的函数，而$\boldsymbol{\theta}$是需要学习的，故也可以看成是变量。
所以在训练时，$\boldsymbol{x}$是已知的，$\boldsymbol{t}$就只是$\boldsymbol{\theta}$的函数了。
$\boldsymbol{\theta}$不同，预测出来的$\boldsymbol{t}$就不同。
所以数据集的每一个数据点都可以得到一个预测值，与实际的结果集中对应的结果之间存在误差，称之为损失。
而最常用的是平方损失，及预测值和实际值的差的平方：
$$L(t_n,f(x_n|\boldsymbol{\theta}))=(t_n-f(x_n|\boldsymbol{\theta}))^2$$
因为$L$损失函数依赖于实际值和预测值，故写成$L(t_n,f(x_n|\boldsymbol{\theta}))$这种形式。

这仍然不能评价模型的好坏，因为每个数据的损失不一样。
因此，训练时需要对每个损失求期望，求得的值我们称之为风险。
风险函数这时已经只是参数的函数了（当训练样本已经确定时）。
使得风险函数最小的参数值就是模型需要的参数了。

此时对风险函数求n次偏导数并另其等于0，可以得到n个方程组，解出参数$\boldsymbol{\theta}$。
当模型为式$(1.1)$时，$\boldsymbol{\theta}$的求法称为最小二乘法。

$\omega_0+\omega_1x$是线性模型的线性响应，而类似
$$\omega_0+\omega_1x+\omega_2x^2+\omega_3x^3+...\tag{2.1}$$
是线性模型的非线性响应。
为什么是线性模型？因为它是参数之间的线性组合，例如正态分布中的$\mu$和$\sigma^2$就不是线性组合的关系，所以u不是线性模型。
而线性响应是指数据集到结果集之间的映射关系呈现线性关系，非线性响应就不是了。
例如刚才的例子$(2.1)$叫做多项式函数。甚至更复杂的可以是
$$\omega_0+\omega_1x+\omega_2sin(\frac{x-a}{b})$$
但是值得说的是，此处多了两个参数$a$和$b$，因此它不是线性模型，但是我们假定已知其值，这时可以使用最小二乘法算出三个参数的值。
